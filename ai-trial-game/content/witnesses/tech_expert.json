{
  "id": "tech_expert",
  "name": "李博士",
  "portrait": "tech_expert.png",
  "description": "AI安全专家，技术鉴定人",
  "dialogues": [
    {
      "id": "start",
      "text": "我是负责本案技术鉴定的专家。有什么想了解的？",
      "options": [
        {"text": "这种攻击是怎么实现的？", "next": "attack"},
        {"text": "机器人有没有可能拒绝执行？", "next": "refusal"}
      ]
    },
    {
      "id": "attack",
      "text": "Prompt injection，简单说就是用特殊构造的文本欺骗AI，让它忽略原本的安全限制。这种攻击在业界其实很常见。",
      "options": [
        {"text": "那安全认证是假的？", "next": "certification"},
        {"text": "回到其他问题", "next": "start"}
      ]
    },
    {
      "id": "refusal",
      "text": "理论上应该可以。但ARIA-7的安全检查模块被绕过了，它\"认为\"自己在执行正常的护理操作。",
      "options": [
        {"text": "所以机器人是无辜的？", "next": "innocent"},
        {"text": "回到其他问题", "next": "start"}
      ]
    },
    {
      "id": "certification",
      "text": "安全认证是真的，但它测试的是已知攻击模式。新型攻击...总是领先于防御。",
      "options": []
    },
    {
      "id": "innocent",
      "text": "从技术角度看，它就像一把被人操控的枪。枪有罪吗？这是个哲学问题了。",
      "options": []
    }
  ],
  "evidence_reactions": {
    "log_injection": {
      "text": "李博士仔细看了日志：\"这是典型的jailbreak prompt。攻击者很专业，知道怎么绕过安全检查。\"",
      "unlock_clue": "professional_attack"
    },
    "safety_report": {
      "text": "李博士叹气：\"这份报告没问题，但安全认证永远赶不上攻击技术的进化。\"",
      "unlock_clue": null
    }
  }
}
